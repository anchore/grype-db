package main

import (
	"fmt"
	"strconv"
	"strings"

	"github.com/anchore/grype-db/cmd/build-cve-set-lookup/termset"
)

type lookups struct {
	termsToProducts map[string][]*product
	allProducts     []*product
}

func (l lookups) termTotals() termset.Set {
	termCounts := termset.Set{}
	for _, p := range l.allProducts {
		termCounts.AddAll(p.terms)
	}
	return termCounts
}

func (l lookups) removeProductTerm(p *product, term string) {
	p.terms.Remove(term)
	p.weights.Remove(term)
	products := remove(l.termsToProducts[term], p)
	if len(products) == 0 {
		delete(l.termsToProducts, term)
	} else {
		l.termsToProducts[term] = products
	}
}

func buildLookups(records []rec) lookups {
	termsToProducts := map[string][]*product{}
	byID := map[string]*product{}

	for _, r := range records {
		if strings.Contains(r.cpe, "backstage") {
			fmt.Print()
		}

		terms := parseTerms(r)

		// reset term counts to 1; counts are based on # of CVEs including the term,
		// not the number of times terms appear
		terms.SetAll(1)

		// CPE terms need to be considered, too
		cpeTerms := parseCpeTerms(r.cpe)
		cpeTerms.SetAll(1)

		terms.AddAll(cpeTerms)

		yearStr := strings.Split(r.cve, "-")[1] // "cve-YEAR-number"
		year := int(logget(strconv.ParseInt(yearStr, 10, 64)))

		existing := byID[r.cpe]
		if existing == nil {
			prod := &product{
				cpe:        r.cpe,
				cpeTerms:   cpeTerms,
				cveMaxYear: year,
				cves:       termset.New(r.cve),
				terms:      terms,
			}
			byID[r.cpe] = prod
			for term := range prod.terms {
				termsToProducts[term] = append(termsToProducts[term], prod)
			}
			continue
		}

		// add the cve
		existing.cves.Add(r.cve)
		existing.terms.AddAll(terms)

		if existing.cveMaxYear < year {
			existing.cveMaxYear = year
		}
	}

	var allProducts []*product
	for _, p := range byID {
		// set weights for all terms to 1 initially
		p.weights = termset.New(p.terms.List()...)
		allProducts = append(allProducts, p)
	}

	// at this point, we have all the CPEs with all the text terms used to describe them
	// including the number of times the terms were used, but we need to reduce the number of terms
	l := lookups{
		termsToProducts: termsToProducts,
		allProducts:     allProducts,
	}

	// normalize the term frequency based on the total number of products with each term -- e.g. if
	// only one product has the term 3 times, and the term is only seen 3 times, this would be weighted to 1;
	// but if the term is seen 6 times, but only 3 times for this product, it would be .5
	l = weightByOverallFrequency(l, .95)

	// weight the term based on the total number of entries per-product -- e.g. if
	// every entry has the term "foo", this would get weighted to 1; but if 1 out of 10 has it, it would be .1
	l = weightByProductPercentage(l, .95)

	// weight the by latest year a term is seen
	l = weightByYear(l)

	// weight all terms in the ID higher
	l = weightByIDTerms(l, 3)

	// only keep the top X terms for each product
	// l = keepTopTerms(l, 10)

	// remove terms used in < X percentage of the product descriptions describing a single product
	l = removeLowWeightTerms(l, .1)

	// remove terms used in other, completely textually unrelated products
	// l = removeTermsFromUnrelatedCPEs(l)

	// remove the top X terms across all inputs
	// l = removeHighFrequencyTerms(l, 1000)

	return l
}
